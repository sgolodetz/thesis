\chapter{Concluding Remarks and Further Work}
\label{chap:conclusions}

%---
\section{Concluding Remarks}

This thesis has demonstrated the effectiveness of partition forests as an editable image representation for segmentation and multi-organ identification in computerised tomography (CT) scans of the abdomen. Despite obtaining promising results, existing multi-organ identification techniques, such as those of Campadelli et al.\ \cite{campadelli09} and Shimizu et al.\ \cite{shimizu07}, have as yet focused only on identifying the organs as automatically as possible, whilst not providing the user with an ability to edit the results produced. In my view, such an ability remains important when the treatment of real people is involved, because unless organ identification techniques become $100\%$ accurate in the future (a somewhat unlikely prospect), medics will always need to confirm that a feature identification result is accurate and edit it when it is not.

The approach I have proposed in this thesis is designed with this in mind; the aim has been to develop a system that can produce good multi-organ identification results whilst still making it easy for the user to manipulate the output. Partition forests are a useful data structure for both purposes -- by representing an image hierarchically, they not only make it easier to design feature identification algorithms that work at a higher level than the raw pixels, they also allow the end result to be represented in a way that is efficient and easy for the user to edit. To support this intended usage, I developed a set of partition forest editing algorithms in Chapter~\ref{chap:ipfs}, together with selection and multi-feature selection data structures to represent the identified features.

TODO

%---
\section{Further Work}

\iffalse

TODO: The list below contains some of the things I should mention.
%
\begin{itemize}
\item Investigate different types of edge-preserving smoothing more thoroughly -- anisotropic diffusion is good, but there are alternatives.
\item Look at whether the watershed algorithm currently being used is appropriate -- compare it to the one in ITK, for instance.
\item Analyse the differences between the waterfall algorithms in more detail.
\item Look at the forest construction process in more detail -- which parameters are best for which types of image? Are there better approaches than watershed/waterfall?
\item Look at the weights given to the edges when performing the waterfall -- are they the right option, or should e.g.~volume dynamics be used?
\item Focus more on the type of image used -- if contrast images work better, make sure those are the ones obtained.
\item Use level sets as part of feature identification. (The current approach is good for finding roughly where to look, but a better approach is needed to actually get the contours.)
\item Work on getting the boundaries `right' between e.g.~spine and ribs.
\item Make the feature identifiers more \emph{robust}. The key issue here is being able to check how you're doing as you go along, otherwise the compile/link/test cycle takes ages on multiple series. Evidently the implementation needs to be faster (see parallelization below). An ideal setup would involve having multiple copies of the program running in parallel on \emph{specific test series} every time a change is made (think distributed computing). (Otherwise, the temptation is always to find series that give good results.)
\item Rather than using ad-hoc thresholds in the feature identifiers, either make them fuzzy or base them on the images involved somehow.
\item Look at ways of parallelizing the implementation (e.g.~using CUDA) -- particularly important for things like visualization.
\end{itemize}

\fi

My research has highlighted a number of potential areas that seem suitable for further work. These can be broadly placed under the headings of \emph{segmentation refinement}, \emph{feature identification refinement} and \emph{parallelization}. The first two are effectively a continuation of the two major strands of work discussed in this thesis; parallelization is important to speed up the implementation of the algorithms for the purposes of more rapid testing and (hopefully) future deployment in a clinical setting.

\subsection{Segmentation Refinement}

There are a number of aspects of the segmentation process used to construct partition forests that present opportunities for further work:
%
\begin{itemize}

\item The parameters to the anisotropic diffusion process, used during the pre-processing phase of the segmentation process to smooth images whilst preserving edges, are currently specified manually by the user, with reasonable defaults provided ($20$ iterations, a conductance of $1.0$ and a time step of $0.0625$). The defaults produce good results in many cases, but a better segmentation is sometimes obtainable by varying the parameters. It would be interesting to look at ways of deriving suitable parameters for a given input volume -- for example, the smoothing could be varied depending on the amount of noise in an image volume. (It might also be useful to take into account the thickness of the axial slices.)

\item The rainfalling algorithm I used to implement the watershed transform (that of Meijster and Roerdink \cite{meijster98}) works well, but it is only one of a number of implementation possibilities (see e.g.~\cite{roerdink01}). Moreover, different watershed algorithms generally produce slightly different results. It would therefore be interesting to investigate whether some watershed algorithms produce more `useful' lowest branch layers of the partition forest than others -- their usefulness could perhaps be evaluated in terms of the usefulness of the overall partition forest produced by a particular combination of watershed and waterfall algorithms, which could in turn be evaluated in terms of how easy it is to identify individual features in the result. (One of the first challenges in this case is evidently to devise some sort of performance metric that can be used to evaluate the different algorithms quantitatively.)

%\item TODO: Look at why the `tweaked' version of Chris's algorithm works better than the `correct' one.

\item As mentioned in \S\ref{subsec:segmentation-waterfall-practicalalgorithms}, there is more than one way of assigning weights to the edges of the region adjacency graphs that are processed as part of the waterfall algorithm. The existing implementation in \emph{millipede} uses the `lowest pass point' approach, which has produced quite good results for the purposes of this thesis, but it would be interesting to investigate the effects of using other criteria, such as the volume extinction values described in \cite{vachier95} or the visually significant dynamics described in \cite{climent06}.

\end{itemize}

\subsection{Feature Identification Refinement}

TODO

\subsection{Parallelization}

Since one of the reasons for trying to automate the segmentation and feature identification tasks is to save users time, achieving good performance is important when developing systems that are intended to see clinical use. For that reason, it would be beneficial to investigate ways of implementing some of the more time-consuming algorithms used in \emph{millipede} in parallel.

In particular, a number of parallel algorithms exist for the watershed transform (e.g.~\cite{bieniek97,moga98}) and, in addition to implementing one, it would be interesting to see whether graph-based waterfall algorithms (which are, after all, merely a series of graph-based watershed transforms) can be similarly parallelized. Visualization is another area which is especially susceptible to parallelization -- indeed, recent GPU-based, massively parallel implementations of single material marching cubes (which can be iterated to extract multiple isosurfaces), such as \cite{dyken08}, have been made to run in real-time on normal consumer hardware.

Another potential target for a massively parallel implementation on the GPU is the anisotropic diffusion process. Despite using a robust, CPU-based implementation from the Insight Toolkit \cite{itk}, this is currently the most costly part of the entire segmentation process (it has to do a lot of work, so this isn't surprising). It would be interesting to try re-implementing the process using a parallel computing architecture like CUDA \cite{cuda} to try and improve performance.
