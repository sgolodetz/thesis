\chapter[Concluding Remarks and Further Work]{Concluding Remarks and\\Further Work}
\label{chap:conclusions}

%##################################################################################################
\section{Concluding Remarks}
%##################################################################################################

An ability to edit the results of segmentation and feature identification algorithms will always be important when the treatment of real people is involved, because unless identification techniques become $100\%$ accurate in the future (a somewhat unlikely prospect), medics will always need to confirm that a program's output is correct before taking potentially critical decisions based upon it.

In that context, this thesis has demonstrated the potential of partition forests as an editable image representation for segmentation and multi-organ identification in computerised tomography (CT) scans of the abdomen. It has done so by making the following specific contributions (these are cross-referenced with the contributions claimed in the introduction, using bracketed numbers, for the purpose of increased clarity):
%
\begin{itemize}

\item The development of a novel editing framework for partition forests (see Chapter~\ref{chap:ipfs}), consisting of a comprehensive set of mutating algorithms for partition forests $(1)$, and subtree-based partition forest selection and multi-feature selection data structures $(2)$. Taken together, these allow the user to completely rearrange the partition forest in any way they see fit, and edit feature identification results in an intuitive and efficient manner. Prior to the development of these mutating algorithms, partition forests, once built, were a static data structure that could not be edited further; these algorithms turn it into a dynamic data structure, lessening the importance of attaining the perfect partition forest during the initial construction process.

\item The development of two watershed-based partition forest construction methods (see \S\ref{sec:segmentation-ipfconstruction}), which allow a single partition forest to be constructed for an image volume $(4)$, regardless of whether the entire volume is segmented in 3D (the \emph{volume-at-once} method), or individual slices are segmented in 2D before combining the results (the \emph{subvolume} method). The subvolume method is particularly novel -- existing methods (e.g.~\cite{yu02,marfil07}) can construct a partition forest by segmenting an entire volume at once, even if they do not use the specific approach taken by the volume-at-once method described herein, but there is no algorithm in the literature that enables a partition forest to be constructed using slice-by-slice segmentation. Such an ability can be useful if the slices in an image series are particularly thick, as the results of 2D segmentation tend to be better than those of 3D in such cases.

\item The development of novel feature identification techniques $(5)$ for 2D and 3D abdominal CT scans (see Chapter~\ref{chap:featureid}). Whilst further refinement is necessary to improve the performance of these techniques in the general case, as observed in the previous two chapters, their current level of performance nevertheless demonstrates the potential of this approach for automated feature identification in the future.

\end{itemize}
%
In addition to these primary contributions, I also developed a new, implementation-invariant algorithm for the waterfall transform $(3)$ on minimum spanning trees (see \S\ref{subsubsec:segmentation-waterfall-myalgorithm}), designed to robustly handle minimal plateaux in the minimum spanning tree, as neither Marcotegui's waterfall algorithm \cite{marcotegui05}, nor a new tree-based algorithm devised by my colleague Chris Nicholls \cite{nicholls09}, do so.

I implemented two segmentation, feature identification and 3D visualization systems, known as \emph{centipede} and \emph{millipede}, to demonstrate the practical effectiveness of the techniques described in this thesis $(6)$. As assessed in Chapter~\ref{chap:assessment}, these were both substantial, medium-sized systems. In particular, \emph{millipede} (the final system), is highly portable, having been successfully used on Microsoft Windows, Linux (Ubuntu) and Mac OS X (Snow Leopard). I anticipate that it should provide a sound basis for further work in this area.

%##################################################################################################
\section{Further Work}
\label{sec:conclusions-furtherwork}
%##################################################################################################

\iffalse

TODO: The list below contains some of the things I should mention.
%
\begin{itemize}
\item Investigate different types of edge-preserving smoothing more thoroughly -- anisotropic diffusion is good, but there are alternatives.
\item Look at whether the watershed algorithm currently being used is appropriate -- compare it to the one in ITK, for instance.
\item Analyse the differences between the waterfall algorithms in more detail.
\item Look at the forest construction process in more detail -- which parameters are best for which types of image? Are there better approaches than watershed/waterfall?
\item Look at the weights given to the edges when performing the waterfall -- are they the right option, or should e.g.~volume dynamics be used?
\item Focus more on the type of image used -- if contrast images work better, make sure those are the ones obtained.
\item Use level sets as part of feature identification. (The current approach is good for finding roughly where to look, but a better approach is needed to actually get the contours.)
\item Work on getting the boundaries `right' between e.g.~spine and ribs.
\item Make the feature identifiers more \emph{robust}. The key issue here is being able to check how you're doing as you go along, otherwise the compile/link/test cycle takes ages on multiple series. Evidently the implementation needs to be faster (see parallelization below). An ideal setup would involve having multiple copies of the program running in parallel on \emph{specific test series} every time a change is made (think distributed computing). (Otherwise, the temptation is always to find series that give good results.)
\item Rather than using ad-hoc thresholds in the feature identifiers, either make them fuzzy or base them on the images involved somehow.
\item Look at ways of parallelizing the implementation (e.g.~using CUDA) -- particularly important for things like visualization.
\end{itemize}

\fi

My research has highlighted a number of potential areas that seem suitable for further work. These can be broadly placed under the headings of \emph{segmentation refinement}, \emph{feature identification refinement} and \emph{parallelization}. The first two are effectively a continuation of the two major strands of work discussed in this thesis; parallelization is important to speed up the implementation of the algorithms for the purposes of more rapid testing and (hopefully) future deployment in a clinical setting.

\subsection{Segmentation Refinement}

There are a number of aspects of the segmentation process used to construct partition forests that present opportunities for further work:
%
\begin{itemize}

\item The parameters to the anisotropic diffusion process, used during the pre-processing phase of the segmentation process to smooth images whilst preserving edges, are currently specified manually by the user, with reasonable defaults provided ($20$ iterations, a conductance of $1.0$ and a time step of $0.0625$). The defaults produce good results in many cases, but a better segmentation is sometimes obtainable by varying the parameters. It would be interesting to look at ways of deriving suitable parameters for a given input volume -- for example, the smoothing could be varied depending on the amount of noise in an image volume. (It might also be useful to take into account the thickness of the axial slices.)

\item The rainfalling algorithm I used to implement the watershed transform (that of Meijster and Roerdink \cite{meijster98}) works well, but it is only one of a number of implementation possibilities (see e.g.~\cite{roerdink01}). Moreover, different watershed algorithms generally produce slightly different results. It would therefore be interesting to investigate whether some watershed algorithms produce more `useful' lowest branch layers of the partition forest than others -- their usefulness could perhaps be evaluated in terms of the usefulness of the overall partition forest produced by a particular combination of watershed and waterfall algorithms, which could in turn be evaluated in terms of how easy it is to identify individual features in the result. (One of the first challenges in this case is evidently to devise some sort of performance metric that can be used to evaluate the different algorithms quantitatively.)

%\item TODO: Look at why the `tweaked' version of Chris's algorithm works better than the `correct' one.

\item As mentioned in \S\ref{subsec:segmentation-waterfall-practicalalgorithms}, there is more than one way of assigning weights to the edges of the region adjacency graphs that are processed as part of the waterfall algorithm. The existing implementation in \emph{millipede} uses the `lowest pass point' approach, which has produced quite good results for the purposes of this thesis, but it would be interesting to investigate the effects of using other criteria, such as the volume extinction values described in \cite{vachier95} or the visually significant dynamics described in \cite{climent06}.

\end{itemize}

\subsection{Feature Identification Refinement}

There are many ways in which the feature identification techniques described in this thesis could benefit from further work (in particular, see Chapter~\ref{chap:assessment} for a detailed, per-identifier discussion), but these are some of the most important:
%
\begin{itemize}

\item A number of the identifiers currently suffer from flooding beyond the desired feature boundary due to weak edges in the underlying image. This is a common trait of region growing algorithms; a potentially better approach in the longer-term is to use the partition forest to identify initial seeds for these features, and then use those seeds to initialise a level sets approach. Whilst this hybrid approach will not be straightforward from an automation perspective (level sets approaches often require a significant amount of tuning to produce the desired results), it may be possible to use it to prevent flooding by carefully controlling the curvature of the resulting contours.

\item The way in which the ribs are localized needs to be improved. The current approach works relatively well in many cases, but sometimes fails to exclude small, bright rib-like regions in the images from consideration. A better approach might involve detecting the outside of the body in the images and ensuring that the perpendicular distance between a rib and this outer contour is quite small.

\item Many of the feature identifiers currently use empirically-determined grow conditions that depend on somewhat arbitrary mean grey value thresholds. A better approach might be (a) to derive the threshold values from the specific images under consideration in each case, and (b) to base the grow conditions on more than just the mean grey values involved.

\item At present, it is difficult to track the way in which changes to the feature identifiers affect their robustness across multiple image series. The current testing cycle involves changing feature identifiers in the code, compiling and linking, and then running the identifiers on a number of series to test their effectiveness. This consumes a vast amount amount of time (of the order of hours), regardless of the size of change made, making feature identification refinement a prolonged and frustrating task. There is hence a strong incentive to test minor changes on only small numbers of series, with clear consequences for overall robustness. It is essential that further work tackle this issue to avoid this sort of problem. The suggested approach is as follows:
%
\begin{enumerate}

\item To eliminate the time spent compiling and linking, the feature identifiers should be implemented as scripts in an interpreted language for testing purposes. These can be changed at a whim without rebuilding the overall application.

\item To eliminate the incentive to test on small numbers of series, multiple copies of the program (each testing the identifiers on a specific series) should be run in parallel, with quantitative results being automatically written into a database alongside an identifier that uniquely identifies the version of the feature identifier that was tested. A distributed approach would be best for this (funding and implementation complexity permitting!). It should always be possible to obtain the quantitative results and source code corresponding to a particular version of a feature identifier, in order to make it easier to analyse which methods are working and which are not.

\end{enumerate}

\end{itemize}

\subsection{Parallelization}

Since one of the reasons for trying to automate the segmentation and feature identification tasks is to save users time, achieving good performance is important when developing systems that are intended to see clinical use. For that reason, it would be beneficial to investigate ways of implementing some of the more time-consuming algorithms used in \emph{millipede} in parallel.

In particular, a number of parallel algorithms exist for the watershed transform (e.g.~\cite{bieniek97,moga98}) and, in addition to implementing one, it would be interesting to see whether graph-based waterfall algorithms (which are, after all, merely a series of graph-based watershed transforms) can be similarly parallelized. Visualization is another area which is especially susceptible to parallelization -- indeed, recent GPU-based, massively parallel implementations of single material marching cubes (which can be iterated to extract multiple isosurfaces), such as \cite{dyken08}, have been made to run in real-time on normal consumer hardware.

Another potential target for a massively parallel implementation on the GPU is the anisotropic diffusion process. Despite using a robust, CPU-based implementation from the Insight Toolkit \cite{itk}, this is currently the most costly part of the entire segmentation process (it has to do a lot of work, so this isn't surprising). It would be interesting to try re-implementing the process using a parallel computing architecture like CUDA \cite{cuda} to try and improve performance.
