\chapter{Concluding Remarks and Further Work}
\label{chap:conclusions}

%##################################################################################################
\section{Concluding Remarks}
%##################################################################################################

An ability to edit the results of segmentation and feature identification algorithms will always be important when the treatment of real people is involved, because unless identification techniques become $100\%$ accurate in the future (a somewhat unlikely prospect), medics will always need to confirm that a program's output is correct before taking potentially critical decisions based upon it.

In that context, this thesis has demonstrated the potential of partition forests as an editable image representation for segmentation and multi-organ identification in computerised tomography (CT) scans of the abdomen. It has done so by making the following specific contributions (these are cross-referenced with the contributions claimed in the introduction using bracketed numbers, for the purpose of increased clarity):
%
\begin{itemize}

\item The development of a novel editing framework for partition forests (see Chapter~\ref{chap:ipfs}), consisting of a comprehensive set of mutating algorithms for partition forests $(1)$, and subtree-based partition forest selection and multi-feature selection data structures $(2)$. Taken together, these allow the user to completely rearrange the partition forest in any way they see fit, and edit feature identification results in an intuitive and efficient manner. Prior to the development of these mutating algorithms, partition forests, once built, were a static data structure that could not be edited further; these algorithms turn it into a dynamic data structure, lessening the importance of attaining the perfect partition forest during the initial construction process.

\item The development of two watershed-based partition forest construction methods (see \S\ref{sec:segmentation-ipfconstruction}), which allow a single partition forest to be constructed for an image volume $(4)$, regardless of whether the entire volume is segmented in 3D (the \emph{volume-at-once} method), or individual slices are segmented in 2D before combining the results (the \emph{subvolume} method). The subvolume method is particularly novel -- existing methods (e.g.~\cite{yu02,marfil07}) can construct a partition forest by segmenting an entire volume at once, even if they do not use the specific approach taken by the volume-at-once method described herein, but there is no algorithm in the literature that enables a partition forest to be constructed using slice-by-slice segmentation. Such an ability can be useful if the slices in an image series are particularly thick, as the results of 2D segmentation tend to be better than those of 3D in such cases.

\item The development of novel feature identification techniques $(5)$ for 2D and 3D abdominal CT scans (see Chapter~\ref{chap:featureid}). Whilst further refinement is necessary to improve the performance of these techniques in the general case, as observed in the previous two chapters, their current level of performance nevertheless demonstrates the potential of this approach for automated feature identification in the future.

\end{itemize}
%
In addition to these primary contributions, I also developed a new, implementation-invariant algorithm for the waterfall transform $(3)$ on minimum spanning trees (see \S\ref{subsubsec:segmentation-waterfall-myalgorithm}), designed to robustly handle minimal plateaux in the minimum spanning tree, as neither Marcotegui's waterfall algorithm \cite{marcotegui05}, nor a new tree-based algorithm devised by my colleague Chris Nicholls \cite{nicholls09}, do so.

I implemented two segmentation, feature identification and 3D visualization systems, known as \emph{centipede} and \emph{millipede}, to demonstrate the practical effectiveness of the techniques described in this thesis $(6)$. As assessed in Chapter~\ref{chap:assessment}, these were both substantial, medium-sized systems. In particular, \emph{millipede} (the final system), is highly portable, having been successfully used on Microsoft Windows, Linux (Ubuntu) and Mac OS X (Snow Leopard). I anticipate that it should provide a sound basis for further work in this area.

%##################################################################################################
\section{Further Work}
\label{sec:conclusions-furtherwork}
%##################################################################################################

\iffalse

TODO: The list below contains some of the things I should mention.
%
\begin{itemize}
\item Investigate different types of edge-preserving smoothing more thoroughly -- anisotropic diffusion is good, but there are alternatives.
\item Look at whether the watershed algorithm currently being used is appropriate -- compare it to the one in ITK, for instance.
\item Analyse the differences between the waterfall algorithms in more detail.
\item Look at the forest construction process in more detail -- which parameters are best for which types of image? Are there better approaches than watershed/waterfall?
\item Look at the weights given to the edges when performing the waterfall -- are they the right option, or should e.g.~volume dynamics be used?
\item Focus more on the type of image used -- if contrast images work better, make sure those are the ones obtained.
\item Use level sets as part of feature identification. (The current approach is good for finding roughly where to look, but a better approach is needed to actually get the contours.)
\item Work on getting the boundaries `right' between e.g.~spine and ribs.
\item Make the feature identifiers more \emph{robust}. The key issue here is being able to check how you're doing as you go along, otherwise the compile/link/test cycle takes ages on multiple series. Evidently the implementation needs to be faster (see parallelization below). An ideal setup would involve having multiple copies of the program running in parallel on \emph{specific test series} every time a change is made (think distributed computing). (Otherwise, the temptation is always to find series that give good results.)
\item Rather than using ad-hoc thresholds in the feature identifiers, either make them fuzzy or base them on the images involved somehow.
\item Look at ways of parallelizing the implementation (e.g.~using CUDA) -- particularly important for things like visualization.
\end{itemize}

\fi

My research has highlighted a number of potential areas that seem suitable for further work. These can be broadly placed under the headings of \emph{segmentation refinement}, \emph{feature identification refinement} and \emph{parallelization}. The first two are effectively a continuation of the two major strands of work discussed in this thesis; parallelization is important to speed up the implementation of the algorithms for the purposes of more rapid testing and (hopefully) future deployment in a clinical setting.

\subsection{Segmentation Refinement}

There are a number of aspects of the segmentation process used to construct partition forests that present opportunities for further work:
%
\begin{itemize}

\item The parameters to the anisotropic diffusion process, used during the pre-processing phase of the segmentation process to smooth images whilst preserving edges, are currently specified manually by the user, with reasonable defaults provided ($20$ iterations, a conductance of $1.0$ and a time step of $0.0625$). The defaults produce good results in many cases, but a better segmentation is sometimes obtainable by varying the parameters. It would be interesting to look at ways of deriving suitable parameters for a given input volume -- for example, the smoothing could be varied depending on the amount of noise in an image volume. (It might also be useful to take into account the thickness of the axial slices.)

\item The rainfalling algorithm I used to implement the watershed transform (that of Meijster and Roerdink \cite{meijster98}) works well, but it is only one of a number of implementation possibilities (see e.g.~\cite{roerdink01}). Moreover, different watershed algorithms generally produce slightly different results. It would therefore be interesting to investigate whether some watershed algorithms produce more `useful' lowest branch layers of the partition forest than others -- their usefulness could perhaps be evaluated in terms of the usefulness of the overall partition forest produced by a particular combination of watershed and waterfall algorithms, which could in turn be evaluated in terms of how easy it is to identify individual features in the result. (One of the first challenges in this case is evidently to devise some sort of performance metric that can be used to evaluate the different algorithms quantitatively.)

%\item TODO: Look at why the `tweaked' version of Chris's algorithm works better than the `correct' one.

\item As mentioned in \S\ref{subsec:segmentation-waterfall-practicalalgorithms}, there is more than one way of assigning weights to the edges of the region adjacency graphs that are processed as part of the waterfall algorithm. The existing implementation in \emph{millipede} uses the `lowest pass point' approach, which has produced quite good results for the purposes of this thesis, but it would be interesting to investigate the effects of using other criteria, such as the volume extinction values described in \cite{vachier95} or the visually significant dynamics described in \cite{climent06}.

\end{itemize}

\subsection{Feature Identification Refinement}

TODO

\subsection{Parallelization}

Since one of the reasons for trying to automate the segmentation and feature identification tasks is to save users time, achieving good performance is important when developing systems that are intended to see clinical use. For that reason, it would be beneficial to investigate ways of implementing some of the more time-consuming algorithms used in \emph{millipede} in parallel.

In particular, a number of parallel algorithms exist for the watershed transform (e.g.~\cite{bieniek97,moga98}) and, in addition to implementing one, it would be interesting to see whether graph-based waterfall algorithms (which are, after all, merely a series of graph-based watershed transforms) can be similarly parallelized. Visualization is another area which is especially susceptible to parallelization -- indeed, recent GPU-based, massively parallel implementations of single material marching cubes (which can be iterated to extract multiple isosurfaces), such as \cite{dyken08}, have been made to run in real-time on normal consumer hardware.

Another potential target for a massively parallel implementation on the GPU is the anisotropic diffusion process. Despite using a robust, CPU-based implementation from the Insight Toolkit \cite{itk}, this is currently the most costly part of the entire segmentation process (it has to do a lot of work, so this isn't surprising). It would be interesting to try re-implementing the process using a parallel computing architecture like CUDA \cite{cuda} to try and improve performance.
