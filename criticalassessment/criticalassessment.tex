\chapter{Critical Assessment}
\label{chap:assessment}

%##################################################################################################
\section{Chapter Overview}
%##################################################################################################

In Chapter~\ref{chap:introduction}, I listed six original contributions made by my doctoral work. Having now discussed the work underlying the contributions in detail in the preceding chapters, and validated my feature identification work in the previous chapter, the goal of this chapter is to evaluate the usefulness of these contributions both within the context of my doctorate and (where applicable) more widely.

%##################################################################################################
\section{Partition Forest Algorithms}
%##################################################################################################

In \S\ref{sec:ipfs-mutatingalgorithms}, I introduced a comprehensive set of algorithms for editing partition forests. Whilst partition forests themselves have appeared extensively in the imaging literature under a variety of different names (see \S\ref{subsec:background-partitionhierarchies-imagerepresentation}), little research attention has hitherto been devoted to the problem of editing them, with the notable exception of Nacken's work on parent switching (which he calls \emph{connectivity preserving relinking}) \cite{nacken95}. My algorithms as a whole thus fill a notable gap in the existing literature.

An ability to edit partition forests is useful in more than one domain. In image analysis, it allows a user to manage and make corrections to a hierarchical segmentation of an image, as demonstrated by my \emph{centipede} and \emph{millipede} segmentation tools. In hierarchical pathfinding, it allows nested partitions of a routing graph to be rearranged so as to minimise the memory consumed by statically-constructed pathfinding tables (this can be particularly important when developing a pathfinding solution that is to run on a machine with limited memory, such as a games console). In organising hierarchical teams of people (for instance, within a large organisation), it can be used to determine the effects of transferring a team member to another team -- will the original team fall apart because the person being transferred was the lynchpin holding things together?

The algorithms I have developed were designed to integrate particularly well into a graphical user interface. Even the more complex forest operations are undoable, and the algorithms all provide sufficient feedback on what changes they made to allow other parts of the program (such as the partition forest selection and the graphical representation of the partition forest) to react accordingly. Furthermore, all of the algorithms are efficient (as demonstrated by the complexity analysis in \S\ref{sec:ipfs-mutatingalgorithms}) and quick to execute in an interactive setting. The higher-level algorithms, such as non-sibling node merging and parent switching, are particularly helpful from the user's perspective, because they allow complicated forest operations to be performed with only a small amount of user interaction.

My parent switching algorithm itself is potentially an improvement on the one described by Nacken. As discussed in \S\ref{subsec:background-partitionhierarchies-imagerepresentation}, Nacken's approach allows the parent of a child to be switched only when both (a) the child is adjacent to a child of the new parent, and (b) moving the child will not affect the contiguity of the old parent. Whilst the first constraint is inherent, my algorithm circumvents the second constraint by splitting the old parent (and potentially some of its ancestors) where necessary, thus allowing further reasonable parent switches to be performed.

\iffalse
+ Useful both for imaging and hierarchical pathfinding
+ Algorithms carefully pinned down, well-justified, reasonable complexities
+ Parent switching algorithm improves on previous state-of-the-art (Nacken) because it makes extra reasonable parent switches possible
+ Unzipping, zipping, non-sibling node merging have no obvious comparisons in the literature
\fi

%##################################################################################################
\section{Partition Forest Selections}
%##################################################################################################

In \S\ref{sec:ipfs-selections}, I described a novel hierarchical representation for partition forest selections. This is based around the concept that the selection of an individual node in the forest equates to the selection of the subtree beneath it. It is a more compact representation than the obvious leaf-based representation that represents the selection as a set of selected partition forest leaves, and lends itself to more efficient interactive editing. For instance, if the user selects a high-level region in the partition forest and then wants to deselect one of its children, far fewer changes need to be made to the hierarchical representation than the leaf-based one (see Figure~\ref{?}). It also avoids the problems that would be caused by an ad-hoc approach that allowed the user to simply add and remove individual nodes from a list of selected nodes. That approach fails because the representation in question admits the possibility of redundancy in the selection -- that is, a node and its partition forest descendant can both be in the list of selected nodes, implicitly selecting some voxels more than once (see Figure~\ref{?}).

The core algorithms I have developed to maintain this hierarchical representation are both efficient and practically useful -- in particular, they have been implemented in both the \emph{centipede} and \emph{millipede} systems and allow the user to intuitively manage selections of nodes from multiple layers of a partition forest without any slow-down. The listening algorithms are also interesting, as they ensure that a selection is updated in line with changes to its corresponding partition forest, making it easier to integrate partition forest selections into a graphical user interface.

Whilst the partition forest selection data structure is inherently tied to partition forests themselves, the underlying hierarchical representation and algorithms are fully transferable to other partition hierarchies, including even non-complete partition trees. It would be easy, for example, to use the ideas here to represent the selection of parts of a binary space partitioning (BSP) tree, as used for spatial representation in some 3D games (perhaps as part of a program that depicted the BSP representation of a world visually to aid understanding). The selection algorithms thus have applications beyond the limited confines of my doctorate.

\iffalse
+ Usefulness well-justified
+ Algorithms carefully pinned down, reasonable complexities
+ No comparison in the literature
+ Can be reused for feature marking
\fi

%##################################################################################################
\section{New Waterfall Algorithm}
%##################################################################################################

In \S\ref{subsubsec:segmentation-waterfall-myalgorithm}, I described a new tree-based algorithm I developed for the waterfall transform, based on a similarly tree-based waterfall algorithm developed by my colleague Chris Nicholls \cite{nicholls09}. Nicholls' algorithm was designed to be far easier to implement than the reference algorithm we already had available, due to Marcotegui \cite{marcotegui05}, and in particular to lend itself well to implementation in a functional programming language such as Haskell. In this, it achieves its aims brilliantly -- even my C++ implementation of Nicholls' algorithm only required roughly $100$ lines of code, and Chris's implementation in Haskell is even shorter. In addition to being easy to implement, Nicholls' algorithm is also efficient, running in linear time once the initial minimum spanning tree has been constructed.

One slight downside of both the Nicholls and Marcotegui algorithms is in how they handle non-minimal plateaux in the minimum spanning tree on which the algorithms work. Specifically, Nicholls' algorithm produces results that depend on where the minimum spanning tree is rooted (and on the order of the children at each node), and Marcotegui's algorithm produces results that depend on the internal implementation of the priority queue used.

My goal in developing a new algorithm was to make its results invariant to such implementation details. The new algorithm has the following advantages:
%
\begin{itemize}
\item It produces consistent results for any given minimum spanning tree.
\item Like Nicholls' algorithm, it is reasonably easy to implement and runs in linear time.
\item Because it carefully classifies the edges in the minimum spanning tree into different types, it is very useful when implementing other waterfall algorithms -- in particular, I was able to make use of it when implementing the first step of Marcotegui's algorithm.
\end{itemize}
%
There are still some downsides, however:
%
\begin{itemize}
\item Most notably, whilst the algorithm produces consistent results for a given minimum spanning tree, it \emph{cannot} produce consistent results for the graph from which the minimum spanning tree was constructed (this is a general problem for all minimum spanning tree-based implementations, including both the Nicholls and Marcotegui algorithms). The results of the algorithm thus still depend on which minimum spanning tree was constructed from the graph. There is no way around this other than to revert to a slower, graph-based implementation.
\item Whilst the algorithm is still linear, it requires a constant factor more work than Nicholls' algorithm, as it has to perform more than one pass of the tree.
\item The algorithm is somewhat harder to implement than Nicholls' algorithm, although it is still relatively easy to code.
\end{itemize}
%
For these reasons, Nicholls' algorithm is still a generally preferable way of performing a waterfall transform, except when the original graph is known to have only one possible minimum spanning tree (for instance, because it itself is a tree) and consistent handling of minimal plateaux is desired.

\iffalse
# Based on Chris's work
+ Produces results that do not depend on the implementation of key data structures (like Marcotegui's) or where the MST is rooted (like Chris's)
+ Efficient, linear-time algorithm
+ Useful as a first step in other waterfall algorithms, because it carefully pins down the types of different edges
+ Easy to implement
- Produces results that depend on which MST is constructed (all MST-based algorithms, including Chris's, necessarily do this -- the only alternative is to run the waterfall on a graph and accept that it will be slower)
- Requires a constant factor more work than Chris's
- Not as easy to implement as Chris's
- The 'incorrect' version of Chris's actually produces better results than either this one or Chris's 'correct' version(!)
\fi

%##################################################################################################
\section{Partition Forest Construction}
%##################################################################################################

TODO

\iffalse
+ Works for both 2D and 3D segmentation, because it's designed using subvolumes
+ No comparison in the literature
- Relatively slow, primarily because of the significant preprocessing time required (anisotropic diffusion filtering, at least as implemented in ITK, is slow as hell)
- Construction process uses twice as much memory as the finished partition forest
\fi

%##################################################################################################
\section{Feature Identification Techniques}
%##################################################################################################

TODO

\iffalse
+ The feature identifiers are relatively quick to run
+ The 2D ribs identifier is relatively effective, although it can miss ribs in some cases
+ The 2D spine identifier is effective and robust (about 85% of results are pretty good)
+ The 3D spine identifier is relatively effective and robust, but has a tendency to flood into the ribs a bit
+ The 3D spinal canal identifier is relatively effective and robust, although the contour of the spinal canal is by no means perfect yet
+ The 3D kidneys identifier is a reasonable start, but requires further work both to make it more robust and to detect kidneys that are made up of multiple regions 
- Because the current techniques are based on region growing, they suffer from the usual sort of region growing problems; a better approach might be to use the partition forest to find initial features, and then use level sets
- The 3D aorta identifier only works well when the segmentation is particularly good; it needs further work
- The 3D liver identifier is not ideal (although it does start out in the right place), and has a tendency to flood into other features
\fi

%##################################################################################################
\section{Implementation}
%##################################################################################################

As mentioned in the introduction, over the course of my doctorate I developed two separate segmentation, feature identification and visualization systems, called \emph{centipede} and \emph{millipede}. Each of these was a substantial, medium-sized system -- as a rough\footnote{I am aware that lines of code is a somewhat unreliable way of measuring the size of a project, but it does at least allow us to distinguish between small, medium and large projects.} indication of scale, \emph{centipede} (the trial system) had $26000+$ lines of C++ code, and \emph{millipede} (the final system) $32000+$ lines. Additionally, \emph{millipede} was designed to be a highly portable, cross-platform project built using CMake -- to date, it has been successfully used on Microsoft Windows, Linux (Ubuntu) and Mac OS X (Snow Leopard), but it is potentially portable to other platforms as well.

The two systems make a vital contribution to this thesis, because they demonstrate that the approach actually works in practice, and on widely-available consumer hardware. From the perspective of future developers, \emph{millipede} in particular also provides a consistently-designed and easily-maintainable code-base that forms a firm foundation for further research work. The build process has been carefully documented on all three of the platforms mentioned, allowing other developers to pick up the project and make changes without the Herculean struggle often associated with building unfamiliar code.

In terms of assessing the development process itself, the development of \emph{millipede} proceeded far more smoothly than that of \emph{centipede}, for a number of reasons:
%
\begin{enumerate}

\item \textbf{Better Domain Knowledge}. Most importantly, \emph{millipede} was developed with the difficulties of developing \emph{centipede} firmly in mind, which allowed me to avoid many of the pitfalls I had encountered the first time around, whilst delivering a more feature-rich end result. (A few examples of specific improvements were that (a) I was able to redesign my partition forest implementation to be faster and more coherent, (b) I was able to implement segmentation and feature identification in 3D as well as 2D, and (c) I was able to make the user interface more responsive using multithreading.)

\item \textbf{Better Tools}. When developing \emph{centipede}, I made the mistake of insisting on developing my own underlying imaging toolkit rather than using the `off-the-shelf' alternative\footnote{In industry, this folly is commonly referred to as suffering from the `not invented here' syndrome.}, which resulted in the initial development taking far longer than it otherwise would have done; for \emph{millipede}, I chose to use the \emph{Insight Toolkit} (ITK) instead.

\item \textbf{Better Scheduling}. The development of \emph{millipede} progressed under much tighter time constraints than \emph{centipede}, which forced me to construct a much more focused development plan, keeping only essential features and avoiding `feature creep'. The actual time spent on implementing \emph{millipede} itself was roughly six months, compared to roughly a year overall for \emph{centipede}.

\end{enumerate}
%
The lessons I have taken from this are that:
%
\begin{enumerate}

\item It is important to build a trial system, but it is often a good idea to then ditch that trial system and start again with the benefit of hindsight. (This echoes Fred Brooks' well-known advice of `Plan to throw one away; you will, anyhow.' \cite{brooks74}) In this case, rewriting from scratch was by far the best option (if a slightly frustrating one from a development perspective), because it allowed improvements to be made that would have taken far longer to integrate into the existing \emph{centipede} system.

\item It is especially crucial to avoid reinventing the wheel when developing systems of a non-trivial size. The goal of learning more about how things work by redeveloping them yourself is a worthwhile one, but that must not be done during the development of a system that has real-world time constraints. Redoing work that has already been done by somebody else takes time away from new work. Systems you redevelop yourself are also often less reliable than their `off-the-shelf' counterparts, which have been subjected to significant testing in the real world.

\item Having less time can result in a better end result, because you are forced to develop only what is necessary. However, this can only work when you already know what you are developing (which is not usually the case at the start of a research project).

\end{enumerate}

\iffalse
+ Medium-size systems (30k+ lines of code each)
+ Implementation of millipede carefully planned with feature list -- very functional, but no unnecessary 'bells and whistles' / feature creep
+ The implementation of millipede proceeded rapidly and according to schedule
+ Consistent, maintainable code (particularly millipede)
+ millipede is entirely cross-platform -- built using CMake; works on Windows, Linux and Mac
+ Runs on normal consumer desktops / laptops
- Should have used ITK the first time round (instead of suffering from NIH syndrome)
- Should have written centipede to be cross-platform from the start
\fi

%##################################################################################################
\section{Chapter Summary}
%##################################################################################################

In this chapter, each of the original contributions mentioned in the introduction was critically assessed. The final chapter contains some concluding remarks and pinpoints some opportunities for further work.
